# ğŸµ AutoSong â€“ Lyric-Driven Autoregressive Composition

## ğŸ§  Purpose  
AutoSong explores whether a **GPT-style autoregressive Transformer** can compose *complete* pieces of music directly from **full-song lyrics and a genre tag**.  
The model receives the entire lyric script up front, allowing it to decide global form (verseâ€“chorus order, bridge placement, etc.) while it generates audio **continuously**.

## ğŸ¯ Prototype Goals  
| Stage | What we demonstrate |
|-------|---------------------|
| **1. Text â†’ Conditioning** | Encode lyrics & genre with a frozen BERT-like encoder â†’ dense matrix \(M\). |
| **2. Audio â†’ Continuous Latents** | Compress 256 Ã— 256-bin **mel-spectrogram** blocks with a **convolutional autoencoder (AE)**.<br>â€ƒâ€¢ Encoder \(E\): \((T, F)\to (C,16,16)\)<br>â€ƒâ€¢ No quantization â€” latent tensors remain **real-valued**. |
| **3. Transformer Autoreg.** | A causal decoder predicts the next latent patch \(\hat z_{t+1}\) given past latents \(z_{\le t}\) **and** cross-attention to \(M\). |
| **4. Latent â†’ Audio** | AE decoder reconstructs mel frames; Griffinâ€“Lim inverts them to waveform. |

Success is measured by both objective reconstruction (L1 on mels) and **subjective musicality** of new lyrics-conditioned generations.

## ğŸ”„ Data Flow


## ğŸ§® Transformer Mechanics

### Input sequence  
Latent patches are raster-ordered:

\[
z_{t}\in\mathbb R^{C\times16\times16}\quad\Longrightarrow\quad
\tilde z_t=\operatorname{reshape}(z_t)\in\mathbb R^{(C\!\cdot\!256)\times d}
\]

with positional \((\mathbf{p})\) and channel \((\mathbf{c})\) embeddings added.

### Layer stack  
For each block \(l=1..L\):

\[
\begin{aligned}
\mathbf X &\leftarrow \text{LayerNorm}\!\left(\mathbf X+\text{SelfAttn}_l(\mathbf X)\right) \\
\mathbf X &\leftarrow \text{LayerNorm}\!\left(\mathbf X+\text{CrossAttn}_l(\mathbf X,\mathbf M)\right) \\
\mathbf X &\leftarrow \text{LayerNorm}\!\left(\mathbf X+\text{MLP}_l(\mathbf X)\right)
\end{aligned}
\]

### Output regression  
Final linear head predicts next patch:

\[
\hat z_{t+1}=W_{\text{out}}\mathbf X_{t}^{\text{(last)}}\in\mathbb R^{C\times16\times16}
\]

Loss =\(\;\lambda_{\text{L1}}\|\hat z_{t+1}-z_{t+1}\|_1\;+\;\lambda_{\text{GAN}}\mathcal L_{\text{adv}}\).

## ğŸ—ï¸ Latent Autoencoder

### Encoder \(E\)  
Convolutional residual stack â†“â†“ to \((C,16,16)\).  
Each output channel is **unit-Gaussian-regularised** with a small prior loss  

\[
\mathcal L_{\text{prior}}=\alpha\; \mathbb E[z^2]
\]

to keep the latent distribution well-behaved for the Transformer.

### Bottleneck  
No quantisation â€” latents remain continuous, enabling **hi-fi reconstruction**.

### Decoder \(D\)  
Symmetric up-sampling residual blocks with InstanceNorm â†’ mel frame~\((T,F)\).

Reconstruction loss  

\[
\mathcal L_{\text{mel}} = \|D(E(\text{mel}))-\text{mel}\|_1
\]

## ğŸš€ Full-Context Training

* **Segment length**: up to 256 mel frames (â‰ˆ 1.3 s).  
* **Curriculum**: start 64 frames â†’ full 256.  
* **Batch** 4â€“16 (AMP enabled).  
* **Optimizer** AdamW, 1 e-4, cosine decay.  
* **Total loss**

\[
\mathcal L = \mathcal L_{\text{mel}} + \beta\,\mathcal L_{\text{prior}}
           + \gamma\,\mathcal L_{\text{adv}}
           + \delta\,\mathcal L_{\text{feature}}
\]

* **Adversary**: Patch-GAN on mel spectrograms to sharpen transients.

## ğŸ—‚ï¸ Dataset Layout
```plaintext
dataset/
â”œâ”€â”€ song_0001/
â”‚   â”œâ”€â”€ lyrics.txt
â”‚   â””â”€â”€ audio.wav  (â‰¥48 kHz mono)
â”œâ”€â”€ song_0002/
â”‚   â”œâ”€â”€ lyrics.txt
â”‚   â””â”€â”€ audio.wav
â‹®
```
Lyrics are converted to pinyin-tone tokens; audio is resampled, converted to 256-bin mels, and cached.

## Research Log: The Road to Autoregressive Music Generation

1. I started with a pure end-to-end approach: using EnCodec + transformer autoregression.  
   It **didn't work at all**. Looking back, I think it's because EnCodec's latent codes are too high in information entropy, and doing end-to-end music generation is simply too ambitious â€” the structure is too deep and subtle.

2. So I tried scaling the problem down.  
   I thought, "Okay, what if we use an autoencoder to compress the content further, and let the transformer handle high-level composition instead?"  
   I attempted VQ-VAE on top of EnCodec, but that didnâ€™t go well either. Discretizing EnCodec's output just wasnâ€™t stable.

3. Then I realized: maybe we shouldn't compress so aggressively.  
   I switched to MEL spectrograms, aiming to model **continuous sound dynamics** instead. This was much more promising: the autoencoder worked.  
   But VQ-VAE still didnâ€™t â€” not sure exactly why. I eventually gave up on quantization and stuck with a continuous latent space.  
   The audio was now represented as 2D latent patches. I used a GAN to sharpen the output and avoid blurry reconstructions from L1/L2 losses.  
   Visualizing the latents, I noticed they looked like downsampled MELs â€” but with 4 channels showing visibly different structures. That felt like real, meaningful representation.

4. Then came autoregression again.  
   Previously, I was focused on predicting token distributions â€” but that didnâ€™t transfer to continuous data. So I redesigned the logic:  
   - Embeddings became *biases* added to the latent patches  
   - I downsampled them via a simple MLP  
   - Ran them through a transformer stack  
   - And then upsampled back to patches  
   Most importantly, I shifted to **residual prediction**, which offloads capacity and makes learning much more efficient.

Now, things finally look reasonable and actually work.  
We can consider scaling the network and training on more diverse tracks.

---

### Reflection

Machine learning and generative modeling are no joke.  
Everything feels obvious in hindsight, but incredibly hard when you're stuck.  
This process â€” of hitting walls, debugging ideas, and slowly forming intuition â€” is what makes research difficult and meaningful.
