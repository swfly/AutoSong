# ğŸµ AutoSong â€“ Lyric-Driven Autoregressive Composition

## ğŸ§  Purpose  
AutoSong explores whether a **GPT-style autoregressive Transformer** can compose *complete* pieces of music directly from **full-song lyrics and a genre tag**.  
The model receives the entire lyric script up front, allowing it to decide global form (verseâ€“chorus order, bridge placement, etc.) while it generates audio **continuously**.

## ğŸ¯ Prototype Goals  
| Stage | What we demonstrate |
|-------|---------------------|
| **1. Text â†’ Conditioning** | Encode lyrics & genre with a frozen BERT-like encoder â†’ dense matrix \(M\). |
| **2. Audio â†’ Continuous Latents** | Compress 256 Ã— 256-bin **mel-spectrogram** blocks with a **convolutional autoencoder (AE)**.<br>â€ƒâ€¢ Encoder \(E\): \((T, F)\to (C,16,16)\)<br>â€ƒâ€¢ No quantization â€” latent tensors remain **real-valued**. |
| **3. Transformer Autoreg.** | A causal decoder predicts the next latent patch \(\hat z_{t+1}\) given past latents \(z_{\le t}\) **and** cross-attention to \(M\). |
| **4. Latent â†’ Audio** | AE decoder reconstructs mel frames; Griffinâ€“Lim inverts them to waveform. |

Success is measured by both objective reconstruction (L1 on mels) and **subjective musicality** of new lyrics-conditioned generations.

## ğŸ§  New Architecture Overview: Composer â†’ Decoder

AutoSong now adopts a **two-stage generation pipeline**, enabling better separation of structure and sound:

---

### ğŸ§± Stage 1: Composer â€” Text-to-Block Generation

The **Composer** is a Transformer that operates autoregressively over **sentence-level blocks**, each corresponding to a segment of lyrics or silence.

- **Input**: Full-song lyrics, tokenized as pinyin-tone syllables.
- Lyrics are segmented into **blocks**, each annotated with:
  - **Token sequence**: lyric content (or empty).
  - **Finish tag**: whether this block concludes a phrase (e.g., end of sentence or pause).
  - **Length class**: coarse quantization of its expected duration.
  - **Role tag**: section function (e.g., verse, chorus, bridge).

- These features are embedded, projected, and combined with temporal and positional embeddings.
- The Composer autoregressively generates the next block embedding, conditioning on the full lyrics context.

> ğŸ§  The Composer models **global structure** â€” form, alignment, and phrasing â€” entirely in the text domain.

---

### ğŸ¼ Stage 2: Decoder â€” Latent-to-Audio Generation

The **Decoder** translates the Composer's block sequence into **continuous audio**, via autoregression in a compressed latent space:

- A separately trained **autoencoder** compresses mel spectrograms into **real-valued latent patches** (e.g., 4Ã—32Ã—32), enabling high-fidelity and low-entropy representation.
  - The encoder is used to preprocess real audio into training latents.
  - The decoder is used to reconstruct mels from predicted latents.
- The **Decoder Transformer**:
  - Operates over latent patches.
  - Cross-attends to the sequence of block embeddings generated by the Composer.
  - Predicts **residual deltas** between latent steps (rather than raw latents), improving temporal smoothness and learning efficiency.

- Reconstructed mels are inverted to waveform via Griffinâ€“Lim.

---

### ğŸ”„ Summary of the Flow

```text
Lyrics (pinyin) â”€â”€â–¶ Composer â”€â”€â–¶ Sentence Block Embeddings â”€â”€â–¶ Decoder â”€â”€â–¶ Latent Patches â”€â”€â–¶ Audio
```
- Level 1: Composer â€” learns high-level musical planning aligned to text.

- Level 2: Decoder â€” realizes the low-level audio from abstract latent plans.

This two-level hierarchy allows the model to compose structured, expressive music directly from lyrics, while handling sound synthesis separately in a learned continuous domain.


## ğŸ—‚ï¸ Dataset Layout
```plaintext
dataset/
â”œâ”€â”€ song_0001/
â”‚   â”œâ”€â”€ lyrics.txt
â”‚   â””â”€â”€ audio.wav  (â‰¥48 kHz mono)
â”œâ”€â”€ song_0002/
â”‚   â”œâ”€â”€ lyrics.txt
â”‚   â””â”€â”€ audio.wav
â‹®
```
Lyrics are converted to pinyin-tone tokens; audio is resampled, converted to 256-bin mels, and cached.

## Research Log: The Road to Autoregressive Music Generation

1. I started with a pure end-to-end approach: using EnCodec + transformer autoregression.  
   It **didn't work at all**. Looking back, I think it's because EnCodec's latent codes are too high in information entropy, and doing end-to-end music generation is simply too ambitious â€” the structure is too deep and subtle.

2. So I tried scaling the problem down.  
   I thought, "Okay, what if we use an autoencoder to compress the content further, and let the transformer handle high-level composition instead?"  
   I attempted VQ-VAE on top of EnCodec, but that didnâ€™t go well either. Discretizing EnCodec's output just wasnâ€™t stable.

3. Then I realized: maybe we shouldn't compress so aggressively.  
   I switched to MEL spectrograms, aiming to model **continuous sound dynamics** instead. This was much more promising: the autoencoder worked.  
   But VQ-VAE still didnâ€™t â€” not sure exactly why. I eventually gave up on quantization and stuck with a continuous latent space.  
   The audio was now represented as 2D latent patches. I used a GAN to sharpen the output and avoid blurry reconstructions from L1/L2 losses.  
   Visualizing the latents, I noticed they looked like downsampled MELs â€” but with 4 channels showing visibly different structures. That felt like real, meaningful representation.

4. Then came autoregression again.  
   Previously, I was focused on predicting token distributions â€” but that didnâ€™t transfer to continuous data. So I redesigned the logic:  
   - Embeddings became *biases* added to the latent patches  
   - I downsampled them via a simple MLP  
   - Ran them through a transformer stack  
   - And then upsampled back to patches  
   Most importantly, I shifted to **residual prediction**, which offloads capacity and makes learning much more efficient.

Now, things finally look reasonable and actually work.  
We can consider scaling the network and training on more diverse tracks.

---

### Reflection

Machine learning and generative modeling are no joke.  
Everything feels obvious in hindsight, but incredibly hard when you're stuck.  
This process â€” of hitting walls, debugging ideas, and slowly forming intuition â€” is what makes research difficult and meaningful.
